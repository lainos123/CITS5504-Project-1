{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "latex_envs": {
     "current_env": "document",
     "environments": [
      "document"
     ]
    }
   },
   "source": [
    "<style>\n",
    "@media print {\n",
    "    a::after {\n",
    "        content: \" (\" attr(href) \") \";\n",
    "    }\n",
    "}\n",
    "a:link { text-decoration: none; color: blue; }\n",
    "a:visited { color: blue; }\n",
    "</style>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CITS5504 Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Laine Mulvay (22708032) and Mohaed Ismail Khan (24894389)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "1. [Introduction](#1-introduction)\n",
    "    - [1.1 Project Overview](#11-project-overview)\n",
    "    - [1.2 Business Objectives](#12-business-objectives)\n",
    "    - [1.3 Tools and Technologies Used](#13-tools-and-technologies-used)\n",
    "2. [Data Warehouse Design](#2-data-warehouse-design)\n",
    "    - [2.1 Schema Overview](#21-schema-overview)\n",
    "        - [2.1.1 Fact Table Design](#211-fact-table-design)\n",
    "        - [2.1.2 Dimension Tables and Concept Hierarchies](#212-dimension-tables-and-concept-hierarchies)\n",
    "        - [2.1.3 Schema and Justification](#213-schema-and-justification)\n",
    "    - [2.2 StarNet Diagram](#22-starnet-diagram)\n",
    "        - [2.2.1 Visualising the StarNet](#221-visualising-the-starnet)\n",
    "        - [2.2.2 Query Footprints](#222-query-footprints)\n",
    "3. [Data Preparation and ETL](#3-data-preparation-and-etl)\n",
    "    - [3.1 Extract](#31-extract)\n",
    "    - [3.2 Transform](#32-transform)\n",
    "    - [3.3 Load](#33-load)\n",
    "4. [Database Implementation](#4-database-implementation)\n",
    "    - [4.1 Containerised Setup](#41-containerised-setup)\n",
    "    - [4.2 Virtual Environment Isolation](#42-virtual-environment-isolation)\n",
    "    - [4.3 Database Access](#43-database-access)\n",
    "    - [4.4 Reproducibility Features](#44-reproducibility-features)\n",
    "5. [Business Queries and Insights](#5-business-queries-and-insights)\n",
    "    - [5.1 Overview of Business Questions](#51-overview-of-business-questions)\n",
    "    - [5.2 SQL Queries](#52-sql-queries)\n",
    "    - [5.3 Visualisation of Queries](#53-visualisation-of-queries)\n",
    "6. [Association Rule Mining](#6-association-rule-mining)\n",
    "    - [6.1 Methodology](#61-methodology)\n",
    "        - [6.1.1 Algorithm Selection](#611-algorithm-selection)\n",
    "        - [6.1.2 Data Preparation](#612-data-preparation)\n",
    "        - [6.1.3 Parameter Selection](#613-parameter-selection)\n",
    "    - [6.2 Implementation](#62-implementation)\n",
    "    - [6.3 Results and Insights](#63-results-and-insights)\n",
    "    - [6.4 Recommendation](#64-recommendation)\n",
    "7. [Conclusion and Reflection](#7-conclusion-and-reflection)\n",
    "8. [References](#8-references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we designed and implemented a data warehouse to facilitate business intelligence and analytics. This report outlines and discusses the project process, including key insights gained throughout the project. Along with the report, all relevant materials have been included in the `Project1_Fatalities` folder.\n",
    "\n",
    "Within this folder, you will find the following:\n",
    "- Scripts: All Python and SQL scripts used throughout the project, including the ETL process (`etl_process.py`) and database queries (`queries.sql`). These scripts are clearly commented to ensure they are well-structured and easy to understand.\n",
    "- Power BI/Tableau Files: The Power BI/Tableau workbooks used for visualisations are included in the archive folder. Specifically, the file `query_visualisations.pdf` provides the visualisations, and the Tableau workbook query_workbook.twb offers the structured data representation.\n",
    "- CSV Files: All CSV files used for building and populating the database are in the data/raw folder. These include essential datasets such as `bitre_fatalities_dec2024.xlsx` and Population estimates by LGA that contributed to the data warehouse’s creation.\n",
    "- README.md: Outlining the poject repository and how to set up the project on your own machine. This includes the set up of Docker, the local PostgreSQL database, pgAdmin and BI tooling\n",
    "\n",
    "Note: Internal links for this report only work when it is opened at `report/Project1_Report.ipynb`\n",
    "\n",
    "The full directory structure is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "Project1_Fatalities\n",
    ".\n",
    "├── Dockerfile\n",
    "├── README.md\n",
    "├── association_rules/\n",
    "├── data\n",
    "│   ├── geojson\n",
    "│   │   ├── LGA_2021_AUST_GDA94.geojson\n",
    "│   │   ├── SA4_2021_AUST_GDA2020.geojson\n",
    "│   │   └── STE_2021_AUST_GDA2020.geojson\n",
    "│   ├── processed/\n",
    "│   └── raw\n",
    "│       ├── LGA (count of dwellings).csv\n",
    "│       ├── ardd_dictionary_sep2023.pdf\n",
    "│       ├── bitre_fatal_crashes_dec2024.xlsx\n",
    "│       └── bitre_fatalities_dec2024.xlsx\n",
    "├── docker-compose.yml\n",
    "├── images/\n",
    "├── report\n",
    "│   └── Project1_Report.ipynb\n",
    "├── requirements.txt\n",
    "├── scripts\n",
    "│   ├── etl_process.py\n",
    "│   ├── queries.sql\n",
    "│   └── setup.sh\n",
    "├── visualisations\n",
    "│   ├── query_visualisations.pdf\n",
    "│   └── query_workbook.twb\n",
    "└── working_notebooks\n",
    "    ├── ETL_Explained.ipynb\n",
    "    └── Kimballs_steps.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1.2 Business Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary objective of this project is to support decision-making by enabling the ability to ask insightful questions and perform effective data analysis. This will be achieved through the design of a snowflake schema data warehouse, which will be explored in detail, including the rationale behind its choice.\n",
    "\n",
    "As part of the project, we will design six key business questions that leverage different combinations of data across various parts of the schema. These questions will draw from multiple dimensions and measures within the data warehouse, ensuring it can effectively support diverse analytical needs. By structuring the data in a snowflake design, we aim to provide a flexible and efficient framework for answering these queries, while also allowing for future analytical growth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### 1.3 Tools and Technologies Used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project utilises Docker to containerise the environment, providing a consistent setup across development and production. It includes a PostgreSQL database for data storage, with pgAdmin for database management and visualisation. The ETL pipeline runs within a Docker container, leveraging Python virtual environments to manage dependencies and ensure portability. For data visualisation, the project integrates with Power BI and Tableau, which connect to the PostgreSQL database for real-time insights. The use of Docker and virtual environments simplifies setup and ensures that the project can be easily reproduced and scaled across different systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Warehouse Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Schema Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: `/working_notebooks/Kimballs_steps.ipynb` was the workbook used to design the following Schema (by implementing Kimballs 4 Steps)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Fact Table Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data warehouse design includes three fact tables, each serving different analytical purposes:\n",
    "\n",
    "\n",
    "\n",
    "**Fact_Crashes**\n",
    "| Column Name | Description |\n",
    "|-------------|-------------|\n",
    "| `crash_id` (PK) | Unique identifier for the crash event. |\n",
    "| `date_id` (FK) | Reference to `Dim_Date` table. |\n",
    "| `lga_id` (FK) | Reference to `Dim_LGA` table, which itself joind `Dim_State`. |\n",
    "| `state_id` (FK) | Reference to `Dim_State` table. |\n",
    "\n",
    "**Fact_Fatalities**\n",
    "| Column Name | Description |\n",
    "|-------------|-------------|\n",
    "| `fatality_id` (PK) | Unique identifier for each fatality. |\n",
    "| `person_id` (FK) | Reference to `Dim_Person` table. |\n",
    "| `crash_id` (FK) | Reference to `Fact_Crashes` table. |\n",
    "\n",
    "\n",
    " **Fact_Number**\n",
    "| Column Name | Description |\n",
    "|-------------|-------------|\n",
    "| `number_date_id` (PK) | Unique identifier for the daily summary record. |\n",
    "| `date_id` (FK) | Reference to `Dim_Date` table. |\n",
    "| `total_fatalities` | Total number of fatalities on the given date (and optionally by state). This is a `MEASURE`|\n",
    "| `total_crashes` | Total number of crashes on the given date (and optionally by state).  This is a `MEASURE`|\n",
    "\n",
    "The fact tables were designed with clear **grain** definitions:\n",
    "\n",
    "- Fact_Crashes: One row per crash event\n",
    "- Fact_Fatalities: One row per fatality associated with a crash\n",
    "- Fact_Number: One row per date for aggregated metrics. This **pre-aggrigated** fact table **stores measures** for **opimised querying**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Dimension Tables and Conecpt Hierachies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data warehouse incorporates several dimension tables with defined hierarchies:\n",
    "\n",
    "##### **Dimension Tables**\n",
    "\n",
    "**Dim_Date**\n",
    "*Time dimension - contains all date attributes*\n",
    "\n",
    "| Column Name | Description |\n",
    "|-------------|-------------|\n",
    "| `date_id` (PK) | Unique identifier for each date |\n",
    "| `year` | Year of the crash/fatality |\n",
    "| `month` | Month of the crash/fatality |\n",
    "| `day` | Day of the month (e.g. 1-31) |\n",
    "| `day_of_week` | Day of the week (e.g., Monday, Tuesday) |\n",
    "| `is_weekend` | Boolean indicating if the crash occurred on a weekend |\n",
    "\n",
    "\n",
    "**Dim_State**\n",
    "*Geographic dimension - highest level geography*\n",
    "\n",
    "| Column Name | Description |\n",
    "|-------------|-------------|\n",
    "| `state_id` (PK) | Unique identifier for each state |\n",
    "| `state_name` | Name of the state |\n",
    "\n",
    "\n",
    "**Dim_LGA**\n",
    "*Geographic dimension - local government area level*\n",
    "\n",
    "| Column Name | Description |\n",
    "|-------------|-------------|\n",
    "| `lga_id` (PK) | Unique identifier for each LGA |\n",
    "| `lga_name` | Local Government Area name |\n",
    "| `state_id` (FK) | Reference to `Dim_State` table |\n",
    "| `national_remoteness_area` | Area classification based on remoteness |\n",
    "| `dwelling_count` | Number of dwellings in the LGA. This is a `MEASURE` |\n",
    "\n",
    "**Dim_Time**\n",
    "*Time dimension - contains time of day attributes*\n",
    "\n",
    "| Column Name | Description |\n",
    "|-------------|-------------|\n",
    "| `crash_id` (PK) | Unique identifier for the time |\n",
    "| `crash_time` | Exact time of the crash (in timestamp format) |\n",
    "| `time_of_day` | Time of day (e.g., Morning, Afternoon, Evening) |\n",
    "\n",
    "\n",
    "**Dim_Vehicle**\n",
    "*Vehicle dimension - contains vehicle involvement attributes*\n",
    "\n",
    "| Column Name | Description |\n",
    "|-------------|-------------|\n",
    "| `crash_id` (PK) | Unique identifier for vehicle data related to a crash |\n",
    "| `bus_involvement` | Boolean indicating if a bus was involved |\n",
    "| `heavy_rigid_truck_involvement` | Boolean indicating if a heavy rigid truck was involved |\n",
    "| `articulated_truck_involvement` | Boolean indicating if an articulated truck was involved |\n",
    "\n",
    "\n",
    "**Dim_Person**\n",
    "*Person dimension - contains demographic information about people involved*\n",
    "\n",
    "| Column Name | Description |\n",
    "|-------------|-------------|\n",
    "| `person_id` (PK) | Surrogate key made from a combination of `CrashID`, `Age`, `Gender`, `RoadUser` |\n",
    "| `crash_id` (FK) | ID of the crash in which the person was involved |\n",
    "| `gender` | Gender of the individual |\n",
    "| `age` | Age of the individual |\n",
    "| `age_group` | Age group of the individual (e.g., 18-25, 26-40) |\n",
    "| `road_user` | Type of road user (e.g., Pedestrian, Driver, Passenger) |\n",
    "\n",
    "\n",
    "**Dim_Event**\n",
    "*Event dimension - contextual information about the crash*\n",
    "\n",
    "| Column Name | Description |\n",
    "|-------------|-------------|\n",
    "| `crash_id` (PK) | Unique identifier for the event |\n",
    "| `christmas_period` | Boolean indicating if the crash occurred during the Christmas period |\n",
    "| `easter_period` | Boolean indicating if the crash occurred during the Easter period |\n",
    "\n",
    "\n",
    "**Dim_Road**\n",
    "*Road dimension - attributes of the road where the crash occurred*\n",
    "\n",
    "| Column Name | Description |\n",
    "|-------------|-------------|\n",
    "| `crash_id` (PK) | Unique identifier for road data |\n",
    "| `speed_limit` | Speed limit of the road where the crash occurred |\n",
    "| `national_road_type` | Type of road (e.g., highway, local road) |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### **Conceptual Hierarchies**\n",
    "\n",
    "##### Geographic Hierarchy\n",
    "```\n",
    "State → LGA\n",
    "```\n",
    "- This hierarchy allows analysis to drill down from state-level to local government areas\n",
    "- The snowflake schema implementation connects Dim_LGA to Dim_State via `state_id`\n",
    "\n",
    "##### Time Hierarchy\n",
    "```\n",
    "Year → Month → Day\n",
    "```\n",
    "- This hierarchy in Dim_Date enables time-based analysis at multiple levels\n",
    "- Additional temporal attributes  like `day_of_week` and `is_weekend`, while not fitting directly in the main heirachy, support specialised time analyses\n",
    "\n",
    "##### Vehicle Classification Hierarchy\n",
    "```\n",
    "Vehicle Type (Bus, Heavy Rigid Truck, Articulated Truck)\n",
    "```\n",
    "\n",
    "##### Road Classification Hierarchy\n",
    "```\n",
    "Speed Limit ranges\n",
    "National Road Type categories\n",
    "```\n",
    "\n",
    "##### Person Hierarchy\n",
    "```\n",
    "Age Group → Age\n",
    "- Road User Type (categorical classification)\n",
    "- Gender (categorical classification)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Schema and Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Schema Design**\n",
    "We have implemented a **partially normalised Snowflake** Schema design for the datawarehouse. This is outlined in the following figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visal Schema Diagram**\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    %% Fact tables\n",
    "    Fact_Crashes[Fact_Crashes]\n",
    "    Fact_Fatalities[Fact_Fatalities]\n",
    "    Fact_Number[Fact_Number]\n",
    "    \n",
    "    %% Independent dimensions\n",
    "    Dim_Date[Dim_Date]\n",
    "    Dim_State[Dim_State]\n",
    "    \n",
    "    %% Hierarchical dimensions\n",
    "    Dim_LGA[Dim_LGA]\n",
    "    \n",
    "    %% Crash-dependent dimensions \n",
    "    Dim_Person[Dim_Person]\n",
    "    Dim_Event[Dim_Event]\n",
    "    Dim_Road[Dim_Road]\n",
    "    Dim_Vehicle[Dim_Vehicle]\n",
    "    Dim_Time[Dim_Time]\n",
    "    \n",
    "    %% Dimension relationships - Snowflake pattern\n",
    "    Dim_LGA -->|state_id| Dim_State\n",
    "    \n",
    "    %% Fact to independent dimension relationships\n",
    "    Fact_Crashes -->|date_id| Dim_Date\n",
    "    Fact_Crashes -->|state_id| Dim_State\n",
    "    Fact_Crashes -->|lga_id| Dim_LGA\n",
    "    \n",
    "    %% Fact-to-fact relationships\n",
    "    Fact_Fatalities -->|crash_id| Fact_Crashes\n",
    "    \n",
    "    %% Secondary fact relationships\n",
    "    Fact_Number -->|date_id| Dim_Date\n",
    "    \n",
    "    %% Person dimension to Fact_Fatalities\n",
    "    Fact_Fatalities -->|person_id| Dim_Person\n",
    "    \n",
    "    %% Crash-dependent dimensions (unusual pattern where crash_id is PK in dimension tables)\n",
    "    Fact_Crashes -->|crash_id| Dim_Event\n",
    "    Fact_Crashes -->|crash_id| Dim_Road\n",
    "    Fact_Crashes -->|crash_id| Dim_Vehicle\n",
    "    Fact_Crashes -->|crash_id| Dim_Time\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schema Diagram from pgAdmin**\n",
    "\n",
    "![chema Diagram from pgAdmin](../images/erd-pgadmin.png)\n",
    "\n",
    "*Note: In Tableau implementation, we create two separate instances of Dim_State (one called Dim_LGA_State that connects to Dim_LGA and another connected directly to Fact_Crashes) because Tableau does not permit loops in relationship paths. In the actual database, these represent the same physical table.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schema Diagram from Tablau with connected GeoJSON (Geospacial Data)**\n",
    "\n",
    "![Schema Diagram from Tablau](../images/data-source-relations-with-geojson.png)\n",
    "\n",
    "*Note: Tableau implementation with 2 instances of Dim_State as mentioned in note above.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Characteristics of Schema Design**\n",
    "\n",
    "##### 1. Normalised Components\n",
    "\n",
    "The majority of our dimension tables are designed in accordance with normalisation principles. Specifically, the crash-dependent dimensions (Dim_Event, Dim_Road, Dim_Vehicle, and Dim_Time) maintain a one-to-one relationship with the Fact_Crashes table, ensuring data integrity and minimising redundancy. Each of these dimension tables directly relates to a unique crash event, avoiding the need to repeat crash-specific information.\n",
    "\n",
    "##### 2. Partial Normalisation in Dim_Person\n",
    "\n",
    "However, the `Dim_Person` table deviates from full normalisation. It exhibits a transitive dependency between the `age_group` and `age` attributes. This violates the third normal form (3NF), as `age_group` is functionally dependent on `age` rather than the primary key `person_id`.\n",
    "\n",
    "To adhere to 3NF, the table could be restructured as follows:\n",
    "```Dim_Person (person_id, crash_id, gender, age, road_user)```\n",
    "```Dim_Age_Groups (age, age_group)```\n",
    "\n",
    "This separation would eliminate redundancy and potential update anomalies, where an age group change would require multiple updates if age groups were not in a separate table.\n",
    "\n",
    "##### 3. Snowflake Pattern in Geographic Dimensions\n",
    "\n",
    "The snowflake pattern is distinctly observed in the geographic dimensions, specifically in the relationship between `Dim_State` and `Dim_LGA`. Each crash in the `Fact_Crashes` table is associated with a state via `state_id`. While all crashes are linked to a state, only a subset are further associated with a specific local government area (LGA) via `lga_id`.\n",
    "\n",
    "To avoid redundant storage of state information at the LGA level, we've implemented a snowflake structure. `Dim_LGA` maintains a foreign key `state_id` linking it to `Dim_State`. This ensures that state information is stored only once, and can be joined to both crash level data and LGA level data. This structure optimises storage and simplifies updates to state-level information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Justificaiton for Schema Design**\n",
    "\n",
    "The chosen design schema is grounded in several key principles:\n",
    "\n",
    "##### 1. Efficient Geographic Data Handling\n",
    "\n",
    "The snowflake pattern is particularly evident in the geographic dimensions, specifically the relationship between `Dim_State` and `Dim_LGA`. This structure is crucial for our dataset due to the hierarchical nature of geographic data. Each crash is associated with a state, and optionally, a more granular Local Government Area (LGA). By normalising state information into `Dim_State` and linking it to `Dim_LGA` via `state_id`, we avoid redundant storage of state details. This approach is consistent with the principles outlined by Kimball and Ross [[1]](#8-references), who advocate for snowflaking in scenarios with well-defined hierarchies. This design facilitates efficient querying and analysis at both state and LGA levels, crucial for spatial analysis and reporting.\n",
    "\n",
    "There are efficiencies with linking the GeoJSON data to `Dim_LGA` via `lga_name` because then we only dig into that geospatial data table if queried. However, this does make these queries slower. Normalisation was preferred over query efficiency because this saved a lot of storage and we don't expect to use the geospatial data that much.\n",
    "\n",
    "##### 2. Storage Optimisation\n",
    "\n",
    "Storing state information once in `Dim_State` significantly reduces storage requirements, especially considering the potential volume of crash data. Adamson [[2]](#8-references) highlights the storage efficiency benefits of snowflaking, particularly for large dimension tables with low-cardinality attributes like state names.\n",
    "\n",
    "##### 3. Improved Data Maintenance\n",
    "\n",
    "Separating `Dim_State` from `Dim_LGA` simplifies data maintenance. If state-level information changes, we only need to update it in one location, ensuring data consistency across the warehouse. Golfarelli and Rizzi [[3]](#8-references) emphasize the ease of maintenance in normalised dimensions, especially when source data is subject to change.\n",
    "\n",
    "##### 4. Partial Normalisation and Practical Considerations\n",
    "\n",
    "While most of the schema adheres to normalisation principles, the `Dim_Person` table exhibits a 3NF violation due to the transitive dependency between `age` and `age_group`. Ideally, we would separate these attributes into `Dim_Person` and `Dim_Age_Groups` tables. However, in practice, this deviation represents a balance between strict normalisation and performance considerations. In our specific use case, the potential performance overhead of additional joins may outweigh the benefits of full 3NF compliance. This decision aligns with the pragmatic approach discussed by Imhoff et al. [[4]](#8-references), who acknowledge that data warehouse design often involves trade-offs.\n",
    "\n",
    "##### 5. Support for Analytical Flexibility\n",
    "\n",
    "The snowflake design, despite its partial normalisation, offers significant analytical flexibility. It allows users to query data at various levels of granularity, from state-level summaries to detailed LGA analyses. This flexibility is essential for comprehensive reporting and data exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 StarNet Diagram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Visualising the StarNet\n",
    "\n",
    "**StarNet Model**\n",
    "\n",
    "![StarNet Blank](../images/StarNet.png)\n",
    "\n",
    "*Note: Grey nodes are classification attributes that are split up into multiple columns in the datawarehouse. Upon reflection these should have been reduced to one multi-valued column during ETL for simplifications during visualisations*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Query Footprints\n",
    "\n",
    "*Note: the questions behind queries will be discussed in\n",
    "[5.1 Overview of Business Questions](#51-Overview-of-Business-Questions)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StarNet Query Footprints**\n",
    "\n",
    "![StarNet Query Footprints](../images/StarNet-queries.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation and ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we describe and reason through key components of the ELT process.\n",
    "\n",
    "For a complete explanation of the ETL workflow implemented in `/scripts/etl_process.py`, refer to the working notebook at `/working_notebooks/ETL_Explained.ipynb`. This notebook provides a step-by-step walkthrough of the entire process.\n",
    "\n",
    "To avoid redundancy, **we’ve chosen not to include code screenshots here**, as the full notebook is accessible and its use is encouraged for a deeper understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extraction phase involves reading data from various sources:\n",
    "\n",
    "-   `bitre_fatalities_dec2024.xlsx`: Contains fatality and fatality count data.\n",
    "-   `bitre_fatal_crashes_dec2024.xlsx`: Contains crash and crash count data.\n",
    "-   `LGA (count of dwellings).csv`: Contains local government area dwelling counts.\n",
    "\n",
    "We use `pandas` to read these files into DataFrames, specifying sheet names and skipping unnecessary rows.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_dir = os.path.join(\"data\", \"raw\")\n",
    "fatalities_file = os.path.join(data_dir, \"bitre_fatalities_dec2024.xlsx\")\n",
    "crashes_file = os.path.join(data_dir, \"bitre_fatal_crashes_dec2024.xlsx\")\n",
    "dwellings_file = os.path.join(data_dir, \"LGA (count of dwellings).csv\")\n",
    "\n",
    "fatality_df = pd.read_excel(fatalities_file, sheet_name=\"BITRE_Fatality\", skiprows=4)\n",
    "fatality_count_df = pd.read_excel(fatalities_file, sheet_name=\"BITRE_Fatality_Count_By_Date\", skiprows=2)\n",
    "crash_df = pd.read_excel(crashes_file, sheet_name=\"BITRE_Fatal_Crash\", skiprows=4)\n",
    "crash_count_df = pd.read_excel(crashes_file, sheet_name=\"BITRE_Fatal_Crash_Count_By_Date\", skiprows=2)\n",
    "dwelling_df = pd.read_csv(dwellings_file, skiprows=7, header=None, names=[\"lga_name\", \"dwelling_count\", \"extra\"], usecols=[\"lga_name\", \"dwelling_count\"]).iloc[2:-5].reset_index(drop=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation phase includes several key steps:\n",
    "\n",
    "**Data Cleaning:**\n",
    "\n",
    "- *Merging DataFrames*: We merge fatality_df and crash_df on Crash ID to create a comprehensive DataFrame.\n",
    "- Column Name Cleaning: We remove newline characters and handle duplicate column names.\n",
    "- *Duplicate Column Handling*: We identify and remove or rename duplicate columns, accounting for _x and _y suffixes resulting from merges.\n",
    "- *Data Type Adjustments*: We adjust data types as needed, such as converting date strings to datetime objects and ensuring numeric columns are properly formatted.\n",
    "- *Handling Missing/Invalid Data*:\n",
    "    - Invalid values (e.g., -9, \"Undetermined\") are converted to NaN using numpy.nan.\n",
    "    - Missing data is considered when designing relationships and tables, ensuring that dimension and fact tables can handle NULL values where applicable.\n",
    "    - We use dropna() to remove rows with critical missing values (e.g., lga_name).\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Merge and clean crash and fatality dataframes\n",
    "crashxfatality_df = fatality_df.merge(crash_df, on=\"Crash ID\", how=\"left\").reset_index(drop=True)\n",
    "cleaned_cols = crashxfatality_df.columns.str.replace(\"n\", \"\", regex=False)\n",
    "seen = {}\n",
    "final_cols = []\n",
    "\n",
    "# ... (column cleaning logic) ...\n",
    "\n",
    "crashxfatality_df = crashxfatality_df.loc[:, crashxfatality_df.columns.notna()]\n",
    "\n",
    "# ... (duplicate column handling logic) ...\n",
    "\n",
    "crashxfatality_df.columns = [col[:-2] if col.endswith('_y') else col for col in crashxfatality_df.columns]\n",
    "crashxfatality_df = crashxfatality_df.drop(columns=['Time of day'])\n",
    "\n",
    "# Merge and clean crashxfatality_count_df\n",
    "crashxfatality_count_df = fatality_count_df.merge(crash_count_df, on=\"Date\", how=\"left\").loc[:, ~crashxfatality_count_df.columns.duplicated()].reset_index(drop=True)\n",
    "\n",
    "# Create Dimension Tables\n",
    "def create_dim_date(df, date_col):\n",
    "    df['date_id'] = pd.to_datetime(df[date_col])\n",
    "    # ... (date dimension creation logic) ...\n",
    "    return df[['date_id', 'year', 'month', 'day', 'day_of_week', 'is_weekend']].drop_duplicates()\n",
    "\n",
    "dim_date = create_dim_date(crashxfatality_count_df, \"Date\").reset_index(drop=True)\n",
    "dim_state = crashxfatality_df[[\"State\"]].drop_duplicates().rename(columns={\"State\": \"state_id\"})\n",
    "# ... (state dimension creation logic) ...\n",
    "dim_lga = crashxfatality_df[[\"National LGA Name 2021\", \"State\", \"National Remoteness Areas\"]].drop_duplicates().merge(dim_state, left_on=\"State\", right_on=\"state_id\", how=\"left\").rename(columns={\"National LGA Name 2021\": \"lga_name\", \"National Remoteness Areas\": \"national_remoteness_area\"})\n",
    "# ... (lga dimension creation logic) ...\n",
    "dim_time = crashxfatality_df[[\"Crash ID\", \"Time\", \"Time of Day\"]].rename(columns={\"Crash ID\": \"crash_id\", \"Time\": \"crash_time\", \"Time of Day\": \"time_of_day\"}).drop_duplicates().reset_index(drop=True)\n",
    "dim_vehicle = crashxfatality_df[[\"Crash ID\", \"Bus Involvement\", \"Heavy Rigid Truck Involvement\", \"Articulated Truck Involvement\"]].rename(columns={\"Crash ID\": \"crash_id\"}).drop_duplicates().replace(-9, np.nan).reset_index(drop=True)\n",
    "dim_person = crashxfatality_df[[\"Crash ID\", \"Gender\", \"Age\", \"Age Group\", \"Road User\"]].rename(columns={\"Crash ID\": \"crash_id\", \"Age Group\": \"age_group\", \"Road User\": \"road_user\"}).drop_duplicates()\n",
    "# ... (person dimension creation logic) ...\n",
    "dim_event = crashxfatality_df[[\"Crash ID\", \"Christmas Period\", \"Easter Period\"]].rename(columns={\"Crash ID\": \"crash_id\"}).drop_duplicates().reset_index(drop=True)\n",
    "dim_road = crashxfatality_df[[\"Crash ID\", \"Speed Limit\", \"National Road Type\"]].rename(columns={\"Crash ID\": \"crash_id\", \"Speed Limit\": \"speed_limit\", \"National Road Type\": \"national_road_type\"}).drop_duplicates()\n",
    "dim_road[\"speed_limit\"] = pd.to_numeric(dim_road[\"speed_limit\"], errors='coerce').astype(\"Int64\")\n",
    "dim_road[\"national_road_type\"] = dim_road[\"national_road_type\"].replace(\"Undetermined\", pd.NA)\n",
    "dim_road[\"speed_limit\"] = dim_road[\"speed_limit\"].replace(-9, np.nan).reset_index(drop=True)\n",
    "\n",
    "# Create Fact Tables\n",
    "# ... (fact table creation logic) ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The load phase involves writing the transformed DataFrames into PostgreSQL tables:\n",
    "\n",
    "- *Database Connection*: We establish a connection to the PostgreSQL database using sqlalchemy.\n",
    "- *Table Creation*: We use df.to_sql() to create and populate tables, replacing existing tables if necessary.\n",
    "- *Primary and Foreign Key Constraints*: We add primary and foreign key constraints to enforce data integrity and relationships.\n",
    "\n",
    "```python\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "DATABASE_URL = \"postgresql://postgres:postgres@pgdb:5432/datawarehouse\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "tables = {\n",
    "    \"dim_date\": dim_date,\n",
    "    \"dim_state\": dim_state,\n",
    "    \"dim_lga\": dim_lga,\n",
    "    \"dim_time\": dim_time,\n",
    "    \"dim_vehicle\": dim_vehicle,\n",
    "    \"dim_person\": dim_person,\n",
    "    \"dim_event\": dim_event,\n",
    "    \"dim_road\": dim_road,\n",
    "    \"fact_fatalities\": fact_fatalities,\n",
    "    \"fact_crashes\": fact_crashes,\n",
    "    \"fact_number\": fact_number\n",
    "}\n",
    "\n",
    "for table_name, df in tables.items():\n",
    "    df.to_sql(table_name, engine, index=False, if_exists='replace')\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    # ... (primary and foreign key constraint logic) ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "source": [
    "## 4. Database Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Containerised Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **PostgreSQL Container** - Hosts the data warehouse with automatic initialization using environment variables for credentials. Maps container port 5432 to host port 5433 to avoid conflicts.\n",
    "2. **pgAdmin Container** - Provides web-based database management accessible at `http://localhost:5051` with preconfigured admin credentials.\n",
    "3. **ETL Container** - Custom-built Python service that runs the transformation pipeline on startup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Virtual Environment Isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ETL container creates an isolated Python environment:\n",
    "1. A dedicated virtual environment is created at `/app/venv`\n",
    "2. All dependencies are installed from `requirements.txt` into this venv\n",
    "3. The PATH is modified to ensure only venv packages are used\n",
    "4. This guarantees identical package versions across all deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Database Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system provides multiple access methods:\n",
    "\n",
    "##### pgAdmin Web Interface\n",
    "- **URL**: http://localhost:5051\n",
    "- **Login**: admin@admin.com / root\n",
    "- **Server Connection**:\n",
    "  - Host: `pgdb` (container name)\n",
    "  - Port: `5432`\n",
    "  - Username: `postgres`\n",
    "  - Password: `postgres`\n",
    "\n",
    "##### External Applications (Tableau/Power BI)\n",
    "- **Host**: localhost\n",
    "- **Port**: 5433 (mapped from container's 5432)\n",
    "- **Database**: datawarehouse\n",
    "- **Credentials**: postgres / postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Reproducibilty Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-command setup ensures identical environments:\n",
    "\n",
    "```bash\n",
    "docker-compose up --build\n",
    "```\n",
    "\n",
    "*Note: Ensure you are in the `/Project1_Fatalities` directory*\n",
    "\n",
    "This Command:\n",
    "1. Builds the ETL service image with locked dependencies\n",
    "\n",
    "2. Creates the PostgreSQL container with empty database\n",
    "\n",
    "3. Initialises pgAdmin with admin credentials\n",
    "\n",
    "4. Automatically runs the ETL process to:\n",
    "\n",
    "    - Create all database tables\n",
    "    - Establish relationships\n",
    "    - Load transformed data\n",
    "\n",
    "5. Preserves data between runs via Docker volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Business Queries and Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Overview of Business Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We developed a set of questions designed to explore various aspects of the data warehouse, leveraging different hierarchies across multiple dimensions:\n",
    "\n",
    "1. Which local government areas had the most road fatalities each year? \n",
    "\n",
    "2. Which state had the most crashes in 2023?\n",
    "\n",
    "3. What time of day and days of the week are associated with the highest number of fatalities?\n",
    "\n",
    "4. How many fatalities occurred during the Christmas and Easter holiday periods?\n",
    "\n",
    "5. Which types of vehicles are most commonly involved in fatal crashes by year in years after 2010?\n",
    "\n",
    "6. (Bonus) - Average fatalities per 1000 dwelling for local government areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 SQL Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A full list of queries can be found at `/scripts/queries.sql`. The following is those queries applied to the datawarehouse in pgAdmin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 : Which local government areas had the most road fatalities each year?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Query 1 - pgAdmin](../images/query_1_pgadmin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2: Which state had the most crashes in 2023?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Query 2 - pgAdmin](../images/query_2_pgadmin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3: What time of day and days of the week are associated with the highest number of fatalities?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Query 3 - pgAdmin](../images/query_3_pgadmin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note the use of the CUBE function to group by both day_of_week and time_of_day. This allows for calculating fatalities by each (day, time) pair, as well as totals for each day, each time, and a grand total, all in one query. This improves efficiency by eliminating the need for multiple GROUP BY queries and UNIONs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4: How many fatalities occurred during the Christmas and Easter holiday periods?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Query 4 - pgAdmin](../images/query_4_pgadmin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5: Which types of vehicles are most commonly involved in fatal crashes by year in years after 2010?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Query 5 - pgAdmin](../images/query_5_pgadmin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The CUBE function is used to group by both year and vehicle_type, providing counts for each vehicle type per year, along with yearly and overall totals. This simplifies the process, avoiding multiple queries with UNIONs and offering a more scalable solution if additional vehicle types are added in the future.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6: Average fatalities per 1000 dwelling for local government areas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Query 6 - pgAdmin](../images/query_6_pgadmin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note this query defines a new, calcualted measure field (average fatalities per 1000 dwellings) across lgs's.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Visualisation of Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a compilation of Tableau visualisations and dashboards designed to address the key queries. These tools enable us to visualise the data in innovative ways, generating valuable insights that are crucial for informed business decisions.\n",
    "\n",
    "- **1. Which local government areas had the most road fatalities each year?**  \n",
    "  ![Q1 Year Map](../images/vis-Q1-year-map.png)  \n",
    "  ![Q1 Total Map](../images/vis-Q1-total-map.png)\n",
    "  Dashboard:  \n",
    "  ![Q1 Dashboard](../images/vis-Q1-dashboard.png)\n",
    "\n",
    "- **2. Which state had the most crashes in 2023?**  \n",
    "  ![Q2 Visualisation](../images/vis-Q2.png)\n",
    "\n",
    "- **3. What time of day and days of the week are associated with the highest number of fatalities?**  \n",
    "  ![Q3 Visualisation](../images/vis-Q3.png)\n",
    "\n",
    "- **4. How many fatalities occurred during the Christmas and Easter holiday periods?**  \n",
    "  ![Q4 Visualisation](../images/vis-Q4.png)\n",
    "\n",
    "- **5. Which types of vehicles are most commonly involved in fatal crashes by year in years after 2010?**  \n",
    "  ![Q5 Visualisation](../images/vis-Q5.png)\n",
    "\n",
    "- **6. (Bonus) - Average fatalities per 1000 dwelling for local government areas**  \n",
    "  ![Q6 Table](../images/vis-Q6-table.png)  \n",
    "  ![Q6 Map](../images/vis-Q6-map.png)\n",
    "\n",
    "*Note: Navigate to `visualisations/query_visualisations.pdf` to view all visualisations in one document.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Tableau Visualisations](../visualisations/query_visualisations.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: the third page of this PDF is an interactive dashboard (allowing visualisations of multiple years of fatalities totals per LGA) and can be opened in tableau from the file at `/visualisations/query_workbook.twb` if the database has been set up as explained in `README.md`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Association Rule Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Methodology\n",
    "#### 6.1.1 Algorithm Selection\n",
    "We implemented the *Apriori algorithm* [[5]](#8-references) using Python's mlxtend library [[6]](#8-references). The algorithm identifies frequent itemsets through iterative candidate generation and pruning, making it suitable for our dataset of ~50,000 transactions. This approach efficiently handles categorical data while maintaining interpretability for road safety analysis.\n",
    "\n",
    "#### 6.1.2 Data Preparation\n",
    "Key preprocessing steps included:\n",
    "- **Data Cleaning**: Replaced '-9', 'Unknown' and 'Undetermined' values with NaN\n",
    "- **Feature Engineering**:\n",
    "  - Binned numerical `Speed Limit` into 4 categories (Low: 0-60km/h, Medium: 61-80km/h, High: 81-100km/h, Very High: 100+km/h)\n",
    "  - Formatted categorical values as `Feature=Value` pairs\n",
    "- **Transaction Encoding**: Converted each fatality record into a transaction of co-occurring attributes\n",
    "- **Column Selection**: Focused on 6 key attributes:\n",
    "  ```python\n",
    "  ['Road User', 'Age Group', 'Gender', 'Speed Category',\n",
    "   'Time of Day', 'National Road Type']\n",
    "  ```\n",
    "\n",
    "#### 6.1.3 Parameter Selection\n",
    "Final parameters were determined through iterative testing:\n",
    "- **Minimum Support**: 0.05 (5% of transactions)\n",
    "- **Maximum Itemset Length**: 3 items\n",
    "- **Lift Threshold**: >1.0\n",
    "\n",
    "These values were chosen to balance rule significance with computational efficiency, filtering out rare patterns while maintaining actionable insights.\n",
    "\n",
    "### 6.2 Implementation\n",
    "The implementation comprises:\n",
    "- **Script Location**: `/association_rules/association_rule_mining.py`\n",
    "- **Docker Configuration**:\n",
    "  ```yaml\n",
    "  services:\n",
    "    association_rules:\n",
    "      build: .\n",
    "      volumes:\n",
    "        - ./data:/app/data\n",
    "        - ./association_rules/results:/app/association_rules/results\n",
    "  ```\n",
    "- **Reproducibility Steps**:\n",
    "  1. Clone repository\n",
    "  2. Run `docker-compose build association_rules`\n",
    "  3. Execute `docker-compose run --rm association_rules`\n",
    "- **Output**: Generates `association_rules.csv` in `/association_rules/results`\n",
    "\n",
    "### 6.3 Results and Insights\n",
    "#### Top 3 Rules (by Lift):\n",
    "1. **Rule 1**:  \n",
    "   `{Speed Category=Low} → {Time of Day=Day, Road User=Pedestrian}`  \n",
    "   - **Support**: 0.054 (5.4% of transactions)  \n",
    "   - **Confidence**: 0.172 (17.2% accuracy)  \n",
    "   - **Lift**: 2.573  \n",
    "   - **Interpretation**: Pedestrian incidents are 2.57 times more likely than average to occur in low-speed daytime areas.\n",
    "\n",
    "2. **Rule 2**:  \n",
    "   `{Speed Category=Very High} → {National Road Type=National or State Highway, Road User=Driver}`  \n",
    "   - **Support**: 0.050 (5.0%)  \n",
    "   - **Confidence**: 0.378 (37.8%)  \n",
    "   - **Lift**: 2.334  \n",
    "   - **Interpretation**: High-speed driver incidents on highways occur 2.33 times more frequently than expected.\n",
    "\n",
    "3. **Rule 3**:  \n",
    "   `{Time of Day=Day, Speed Category=Low} → {Road User=Pedestrian}`  \n",
    "   - **Support**: 0.054 (5.4%)  \n",
    "   - **Confidence**: 0.308 (30.8%)  \n",
    "   - **Lift**: 2.299  \n",
    "   - **Interpretation**: 30.8% of daytime low-speed crashes involve pedestrians, with 2.3x increased likelihood.\n",
    "\n",
    "#### Key Insights:\n",
    "- Pedestrian safety is strongly associated with low-speed zones\n",
    "- Driver incidents correlate with high-speed highways\n",
    "- Temporal patterns show daytime predominance for pedestrian incidents\n",
    "- Lift values indicate non-random associations across all top rules\n",
    "\n",
    "### 6.4 Recommendations\n",
    "1. **Low-Speed Pedestrian Infrastructure**  \n",
    "   Install raised crossings and pedestrian-activated signals in urban areas with frequent low-speed incidents (supported by Rules 1 & 3's lift >2.29).\n",
    "\n",
    "2. **National Highway Speed Management**  \n",
    "   Implement average-speed cameras and dynamic signage on high-speed corridors (aligned with Rule 2's 37.8% confidence).\n",
    "\n",
    "3. **Daytime Pedestrian Awareness Programs**  \n",
    "   Develop targeted education campaigns for schools and senior communities (informed by Rules 1 & 3's daytime pattern)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion and Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project delivered a functional data warehouse for analysing road safety trends, enabling efficient querying and visualisation of crash and fatality data. The snowflake schema design successfully supported complex analytical questions while maintaining data integrity through partial normalisation. Integration with Tableau and Power BI demonstrated the warehouse's practical utility for spatial and temporal analysis, though several key improvements were identified for future iterations.  \n",
    " \n",
    "Several learnings emerged that could refine the design:  \n",
    "- **Classification attributes** like vehicle types (e.g., bus/truck involvement) were stored as separate boolean columns in `Dim_Vehicle`. Consolidating these into a single categorical column (e.g., \"vehicle_type\") would streamline visualisation workflows in BI tools.  \n",
    "- While most Yes/No fields were converted to booleans (e.g., holiday flags in `Dim_Event`), standardising this conversion earlier in the ETL process would improve consistency.  \n",
    "- The redundancy between `age` and `age_group` in `Dim_Person` could be resolved by fully normalising these into separate tables, trading minor query complexity for better storage efficiency.  \n",
    "\n",
    "These adjustments would better align the schema with dimensional modelling best practices while preserving the snowflake pattern's strengths in handling geographic hierarchies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#cits5504-project-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] R. Kimball and M. Ross, *The Data Warehouse Toolkit: The Definitive Guide to Dimensional Modeling*, 3rd ed. John Wiley & Sons, 2013.\n",
    "\n",
    "[2] C. Adamson, *Star Schema: The Complete Reference*. McGraw-Hill, 2010.\n",
    "\n",
    "[3] M. Golfarelli and S. Rizzi, *Data Warehouse Design: Modern Principles and Methodologies*. McGraw-Hill, 2009.\n",
    "\n",
    "[4] C. Imhoff, N. Galemmo, and J. G. Geiger, *Mastering Data Warehouse Design: Relational and Dimensional Techniques*. John Wiley & Sons, 2003.\n",
    "\n",
    "[5] R. Agrawal and R. Srikant, “Fast Algorithms for Mining Association Rules,” Proceedings of the 20th International Conference on Very Large Data Bases (VLDB), 1994.\n",
    "\n",
    "[6] S. Raschka, “MLxtend: Providing Machine Learning Extensions,” Journal of Open Source Software, vol. 3, no. 24, p. 638, 2018."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
